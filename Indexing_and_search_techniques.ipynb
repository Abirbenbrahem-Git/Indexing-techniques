{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdFmsVufxIGC",
        "outputId": "9493ed47-07f0-4185-d878-9d15a522ba8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def stem_function(file):\n",
        "  from nltk.stem.porter import PorterStemmer\n",
        "  path = \"sample_data/corpus/\"\n",
        "  text=f.read()\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  print(len(tokens))\n",
        "  nt=list(filter(lambda token: token not in stopwords.words('french'), tokens))\n",
        "  print(len(nt))\n",
        "  stemmer = PorterStemmer()\n",
        "  ftokens=[stemmer.stem(token) for token in nt]\n",
        "  return ftokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME3T0Nzn15-0",
        "outputId": "f4009e46-1242-42ef-d329-5d38a31c7beb"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TermF_function(tokens):\n",
        "  tfdict={}\n",
        "  for t in tokens:\n",
        "    if t not in tfdict:\n",
        "        tfdict[t] = 1\n",
        "    elif t in tfdict:\n",
        "        tfdict[t] += 1\n",
        "  return tfdict\n",
        " "
      ],
      "metadata": {
        "id": "EBBnkP6s4NCi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dictionnaire_global(file):\n",
        "  d_globale={}\n",
        "  i=1\n",
        "  for d in file:\n",
        "    d_globale[i]=TermF_function(d)\n",
        "    i=i+1\n",
        "  return d_globale"
      ],
      "metadata": {
        "id": "y7_5sSVy4z2C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def IDF(file):\n",
        "  total_docs = len(file)\n",
        "  idf= {}\n",
        "  for doc in file:\n",
        "        for stem in file:\n",
        "            if stem in idf:\n",
        "                idf[stem] += 1\n",
        "            else:\n",
        "                idf[stem] = 1\n",
        "                for stem in idf:\n",
        "                  idf[stem] = math.log(total_docs/idf[stem])\n",
        "                  return idf"
      ],
      "metadata": {
        "id": "oewDcOEk5TFe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Poids(file):\n",
        "  d1=dictionnaire_global(file)\n",
        "  d2=IDF(file)\n",
        "  for i in d1:\n",
        "    d=d1[i]\n",
        "    for m in d:\n",
        "      d[m]=d[m]*d2[m]\n",
        "    return d1  "
      ],
      "metadata": {
        "id": "v5LiZDQn6e6g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fich_inverse(file):\n",
        "    document=open(\"fich-inv.txt\",\"w\")\n",
        "    l=set(stem_function(file))\n",
        "    for m in l:\n",
        "      document.write(m+\"----\"+file+\"----\"+str(Poids(file))+\"\\n\")\n",
        "    document.close()"
      ],
      "metadata": {
        "id": "JAywl5gu6iE9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Open a file\n",
        "path = \"sample_data/corpus/\"\n",
        "dirs = os.listdir( path )\n",
        "t=[]\n",
        "tfd={}\n",
        "for file in dirs:\n",
        "    t=stem_function(file)\n",
        "    tfd=TermF_function(t)\n",
        "    print(tfd)\n",
        "    dg=dictionnaire_global(file)\n",
        "    print(dg)\n",
        "    idf=IDF(file)\n",
        "    print(idf)\n",
        "    p=Poids(file)\n",
        "    print(p)  \n",
        "    print(fich_inverse(file))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp_LmKqn7YOL",
        "outputId": "63a812a2-9db8-4d61-91a9-73fbf9caa76b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313\n",
            "183\n",
            "{'il': 2, 'interdit': 1, 'fair': 2, 'musiqu': 1, 'plu': 3, 'vingt-quatr': 1, 'heur': 1, 'jour': 1, 'ça': 2, 'finira': 1, 'tort': 1, 'hier': 1, 'soir': 1, 'hindou': 1, 'amnésiqu': 1, 'a': 6, 'mi': 1, 'tou': 2, 'souvenir': 3, 'gross': 2, 'boul': 3, 'or': 1, 'roulé': 1, 'fond': 2, '’': 13, 'corridor': 1, 'pui': 2, 'escali': 1, 'dégringolé': 1, 'renvers': 1, 'monsieur': 2, 'devant': 1, 'loge': 1, 'concierg': 1, 'voulait': 1, 'dire': 1, 'nom': 2, 'rentrant': 1, 'et': 2, 'jeté': 1, 'tête': 1, 'dit': 1, 'place': 1, 'sien': 1, 'mainten': 3, 'voilà': 1, 'tranquil': 1, 'bon': 1, 'petit': 2, 'bout': 1, 'temp': 1, 'tout': 2, 'pri': 1, 'souvien': 2, 'rien': 2, 'parti': 1, 'sanglot': 1, 'tomb': 1, 'grand-pèr': 1, 'paternel': 1, 'judicieux': 1, 'éleveur': 1, 'sauterel': 1, 'homm': 1, 'valait': 1, 'grand': 5, 'chose': 2, 'peur': 1, 'portait': 1, 'bretel': 1, 'mauv': 3, 'sa': 1, 'femm': 1, 'appelait': 1, 'vaurien': 1, 'saurien': 2, 'peut-êtr': 3, 'oui': 1, 'cela': 1, 'croi': 1, 'bien': 1, 'autr': 1, 'est-c': 2, 'sai': 1, 'futilité': 1, 'tiroir': 1, 'miett': 1, 'gravat': 1, 'mémoir': 4, 'je': 1, 'connai': 1, 'fin': 1, 'mot': 4, 'histoir': 1, 'comment': 1, 'est-el': 1, 'fait': 1, 'quoi': 2, 'a-t-el': 1, 'air': 2, 'aura-t-el': 1, 'tard': 1, 'vert': 1, 'vacanc': 1, 'devenu': 1, 'panier': 1, 'osier': 1, 'sanglant': 1, 'mond': 1, 'assassiné': 1, 'dedan': 1, 'étiquett': 1, 'haut': 1, 'ba': 1, 'fragil': 1, 'lettr': 1, 'roug': 1, 'bleue': 1, 'pourquoi': 1, 'enfin': 1, 'grise': 1, 'rose': 1, 'puisqu': 1, 'choix': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'5': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'5': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "313\n",
            "183\n",
            "None\n",
            "63\n",
            "35\n",
            "{'il': 1, 'a': 3, '’': 5, 'appel': 2, 'aimé': 1, 'bienvenu': 1, 'désiré': 1, 'appelé': 1, 'destiné': 1, 'je': 1, 'sai': 2, 'pourquoi': 1, 'donné': 1, 'nom-là': 1, 'mai': 1, 'chanc': 1, 'on': 1, 'pu': 1, 'bon': 1, 'rien': 1, 'mauvais': 1, 'grain': 1, 'détesté': 1, 'méprisé': 1, 'perdu': 1, 'jamai': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'8': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'8': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "63\n",
            "35\n",
            "None\n",
            "48\n",
            "27\n",
            "{'vou': 1, 'je': 1, 'regard': 4, 'vie': 1, 'non': 1, 'plu': 1, 'j': 2, '’': 5, 'aim': 4, 'cela': 1, 'seul': 1, 'voit': 1, 'ceux': 1, 'donnent': 1, 'droit': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'4': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'4': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "48\n",
            "27\n",
            "None\n",
            "96\n",
            "48\n",
            "{'il': 1, 'dit': 4, 'non': 2, 'tête': 1, 'oui': 2, 'coeur': 1, '’': 1, 'aim': 1, 'professeur': 1, 'debout': 1, 'questionn': 1, 'tou': 1, 'problèm': 1, 'posé': 1, 'soudain': 1, 'fou': 1, 'rire': 1, 'prend': 1, 'effac': 1, 'tout': 2, 'chiffr': 1, 'mot': 1, 'date': 1, 'nom': 1, 'phrase': 1, 'pièg': 1, 'malgré': 1, 'menac': 1, 'maîtr': 1, 'sou': 1, 'huée': 1, 'enfant': 1, 'prodig': 1, 'crai': 1, 'couleur': 1, 'tableau': 1, 'noir': 1, 'malheur': 1, 'dessin': 1, 'visag': 1, 'bonheur': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'3': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'3': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "96\n",
            "48\n",
            "None\n",
            "97\n",
            "55\n",
            "{'le': 1, 'tendr': 1, 'dangereux': 1, 'visag': 1, '’': 10, 'amour': 2, 'apparu': 1, 'soir': 1, 'aprè': 1, 'trop': 2, 'long': 1, 'jour': 1, 'c': 1, 'peut-êtr': 3, 'archer': 1, 'arc': 1, 'bien': 1, 'musicien': 1, 'harp': 1, 'je': 2, 'sai': 4, 'plu': 1, 'rien': 1, 'tout': 2, 'a': 2, 'blessé': 3, 'flèche': 1, 'chanson': 1, 'coeur': 1, 'toujour': 1, 'brûlant': 2, 'blessur': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'1': 1}, 5: {'0': 1}, 6: {'.': 1}, 7: {'t': 1}, 8: {'x': 1}, 9: {'t': 1}}\n",
            "{'d': 2.1972245773362196}\n",
            "{1: {'d': 2.1972245773362196}, 2: {'o': 1}, 3: {'c': 1}, 4: {'1': 1}, 5: {'0': 1}, 6: {'.': 1}, 7: {'t': 1}, 8: {'x': 1}, 9: {'t': 1}}\n",
            "97\n",
            "55\n",
            "None\n",
            "273\n",
            "143\n",
            "{'en': 1, 'sortant': 1, '’': 9, 'école': 1, 'rencontré': 4, 'grand': 1, 'chemin': 4, 'fer': 4, 'a': 5, 'emmené': 1, 'tout': 9, 'autour': 7, 'terr': 6, 'wagon': 1, 'doré': 1, 'mer': 5, 'promenait': 1, 'tou': 1, 'coquillag': 1, 'île': 1, 'parfumé': 1, 'pui': 1, 'beaux': 1, 'naufrag': 1, 'saumon': 1, 'fumé': 1, 'au-dessu': 1, 'lune': 2, 'étoil': 2, 'bateau': 2, 'voil': 2, 'partant': 1, 'japon': 1, 'troi': 1, 'mousquetair': 1, 'cinq': 1, 'doigt': 1, 'main': 1, 'tournant': 1, 'manivel': 1, 'petit': 1, 'sous-marin': 1, 'plongeant': 1, 'fond': 1, 'chercher': 1, 'oursin': 1, 'reven': 1, 'voie': 2, 'maison': 2, 'fuyait': 4, 'devant': 1, 'hiver': 2, 'voulait': 2, 'attrap': 1, 'mai': 1, 'mi': 1, 'rouler': 2, 'derrièr': 1, 'écrasé': 1, 'arrêté': 1, 'printemp': 1, 'salué': 1, 'c': 1, 'garde-barrièr': 1, 'bien': 1, 'remercié': 1, 'fleur': 1, 'soudain': 1, 'mise': 1, 'pousser': 2, 'tort': 1, 'traver': 1, 'plu': 1, 'avanc': 1, 'peur': 1, 'abîmer': 1, 'alor': 1, 'revenu': 1, 'pie': 4, 'soleil': 1, 'cheval': 1, 'voitur': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'9': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'9': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "273\n",
            "143\n",
            "None\n",
            "113\n",
            "61\n",
            "{'je': 2, 'sali': 1, 'rue': 3, '’': 8, 'excus': 1, 'homme-sandwich': 1, 'a': 1, 'donné': 1, 'prospectu': 1, 'armé': 1, 'salut': 1, 'jeté': 1, 'là': 1, 'tout': 2, 'froissé': 1, 'ruisseau': 1, 'eau': 1, 'tard': 1, 'couler': 1, 'pardonnez-moi': 1, 'cett': 1, 'offens': 1, 'éboueur': 2, 'vont': 1, 'passer': 1, 'valet': 1, 'mécaniqu': 1, 'effacé': 1, 'alor': 1, 'dirai': 1, 'salu': 2, 'plein': 2, 'ogress': 1, 'charmant': 1, 'comm': 1, 'cont': 1, 'chinoi': 1, 'plantent': 1, 'coeur': 1, 'épée': 1, 'cristal': 1, 'plaisir': 1, 'plaie': 1, 'heureus': 1, 'désir': 1, 'grâce': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'7': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'7': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "113\n",
            "61\n",
            "None\n",
            "274\n",
            "201\n",
            "{'deux': 9, 'quatr': 10, 'huit': 11, 'font': 6, 'seize…': 2, 'répétez': 2, '!': 4, 'dit': 2, 'maîtr': 2, 'mai': 2, 'voilà': 1, '’': 21, 'oiseau-lyr': 2, 'qui': 1, 'pass': 1, 'ciel': 1, 'l': 5, 'enfant': 10, 'voit': 1, 'entend': 3, 'appel': 1, ':': 2, 'sauve-moi': 1, 'joue': 5, 'oiseau': 5, 'alor': 1, 'descend': 1, 'et': 16, 'quatre…': 1, 'lui…': 1, 'seiz': 5, 'est-c': 1, '?': 1, 'il': 1, 'rien': 1, 'surtout': 1, 'trente-deux': 1, 'de': 1, 'tout': 1, 'façon': 1, 'vont': 3, '.': 4, 'a': 2, 'caché': 1, 'dan': 1, 'pupitr': 2, 'tou': 3, 'chanson': 1, 'musiqu': 2, 'tour': 2, 'fichent': 1, 'camp': 1, 'ni': 2, 'un': 1, 'également': 1, 'chant': 1, 'professeur': 1, 'crie': 1, 'quand': 1, 'fini': 1, 'fair': 1, 'pitr': 1, 'autr': 1, 'écoutent': 1, 'mur': 1, 'class': 1, 's': 1, 'écroulent': 1, 'tranquil': 1, 'vitr': 1, 'redevienn': 2, 'sabl': 1, 'encr': 1, 'redevi': 3, 'eau': 1, 'le': 2, 'arbr': 1, 'la': 1, 'craie': 1, 'falais': 1, 'porte-plum': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'2': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'2': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "274\n",
            "201\n",
            "None\n",
            "74\n",
            "56\n",
            "{'être': 2, 'ang': 4, 'c': 2, '’': 10, 'étrang': 5, 'dit': 6, 'âne': 4, 'étrâne': 2, 'cela': 1, 'veut': 2, 'rien': 1, 'dire': 2, 'haussant': 1, 'ail': 1, 'pourtant': 1, 'si': 1, 'quelqu': 1, 'chose': 1, 'plu': 1, '!': 1, 'tapant': 1, 'pie': 1, 'étranger': 1, 'vous-mêm': 1, 'et': 1, 'envol': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'6': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'6': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "74\n",
            "56\n",
            "None\n",
            "285\n",
            "168\n",
            "{'kabyl': 1, 'chapel': 1, 'quai': 1, 'javel': 1, 'homm': 1, 'pay': 2, 'lointain': 1, 'cobay': 1, 'coloni': 1, 'doux': 1, 'petit': 3, 'musicien': 1, 'soleil': 1, 'adolesc': 1, 'port': 2, '’': 5, 'itali': 1, 'boumian': 1, 'saint-ouen': 1, 'apatrid': 1, 'aubervilli': 1, 'brûleur': 1, 'grand': 1, 'ordur': 1, 'vill': 2, 'pari': 1, 'ébouillanteur': 1, 'bête': 1, 'trouvé': 1, 'mort': 1, 'pie': 1, 'beau': 1, 'milieu': 1, 'rue': 1, 'tunisien': 1, 'grenel': 1, 'embauché': 1, 'débauché': 1, 'manoeuvr': 1, 'désoeuvré': 1, 'polack': 1, 'marai': 1, 'templ': 1, 'rosier': 1, 'cordonni': 1, 'cordou': 1, 'soutier': 1, 'barcelon': 1, 'pêcheur': 1, 'baléar': 1, 'bien': 1, 'finisterr': 1, 'rescapé': 1, 'franco': 1, 'déporté': 1, 'franc': 1, 'navarr': 1, 'avoir': 1, 'défendu': 1, 'souvenir': 1, 'vôtre': 1, 'liberté': 1, 'autr': 1, 'esclav': 2, 'noir': 2, 'fréju': 2, 'tiraillé': 1, 'parqué': 1, 'bord': 1, 'mer': 1, 'où': 1, 'peu': 1, 'baignez': 1, 'évoquez': 1, 'chaqu': 1, 'soir': 1, 'locaux': 1, 'disciplinair': 1, 'vieill': 1, 'boît': 1, 'cigar': 1, 'quelqu': 1, 'bout': 1, 'fil': 1, 'fer': 1, 'tou': 2, 'écho': 1, 'villag': 1, 'oiseaux': 1, 'forêt': 1, 'venez': 1, 'capital': 1, 'fêter': 1, 'cadencé': 1, 'prise': 1, 'bastil': 1, 'quatorz': 1, 'juillet': 1, 'enfant': 3, 'sénégal': 1, 'dépatrié': 1, 'expatrié': 1, 'naturalisé': 1, 'indochinoi': 1, 'jongleur': 1, 'innoc': 1, 'couteaux': 2, 'vendiez': 1, 'autrefoi': 1, 'terrass': 1, 'café': 1, 'joli': 1, 'dragon': 1, 'or': 1, 'fait': 1, 'papier': 2, 'plié': 1, 'trop': 1, 'tôt': 1, 'grandi': 1, 'si': 3, 'vite': 1, 'allé': 1, 'dormez': 1, 'aujourd': 1, 'hui': 1, 'retour': 1, 'visag': 1, 'terr': 1, 'bomb': 1, 'incendiair': 1, 'labour': 1, 'rizièr': 1, 'on': 1, 'a': 2, 'renvoyé': 1, 'monnai': 1, 'doré': 1, 'retourné': 1, 'do': 1, 'étrang': 1, 'étranger': 1, 'vou': 1, 'vie': 1, 'mal': 1, 'vivez': 1, 'mourez': 1, '.': 1}\n",
            "{1: {'d': 1}, 2: {'o': 1}, 3: {'c': 1}, 4: {'1': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "{'d': 2.0794415416798357}\n",
            "{1: {'d': 2.0794415416798357}, 2: {'o': 1}, 3: {'c': 1}, 4: {'1': 1}, 5: {'.': 1}, 6: {'t': 1}, 7: {'x': 1}, 8: {'t': 1}}\n",
            "285\n",
            "168\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lecture_req():\n",
        "    requete=input(\"Entrer la requete : \\n\")\n",
        "    return requete"
      ],
      "metadata": {
        "id": "9htdfuU2KwHo"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexation_requete(requete):\n",
        "    t_requete=stem_function(requete)\n",
        "    tfd_requete=TermF_function(t_requete)\n",
        "    print(tfd_requete)\n",
        "    idf_requete=IDF(requete)\n",
        "    print(idf_requete)\n",
        "    p_requete=Poids(requete)\n",
        "    print(p_requete)  \n",
        "    print(fich_inverse(requete))"
      ],
      "metadata": {
        "id": "5HN124eBNSFJ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def similarite(t,file,requete):\n",
        "    Wd=0\n",
        "    Wq=0\n",
        "    wdd=0\n",
        "    RSV=0\n",
        "    t_requete=indexation_requete(requete)\n",
        "    for i in t:\n",
        "        if (i in t_requete):\n",
        "            q=1\n",
        "        else:\n",
        "            q=0\n",
        "        try:\n",
        "            d=Poids(t,file)\n",
        "        except:\n",
        "            d=0\n",
        "        Wd=Wd+(q*d)\n",
        "    for i in t:\n",
        "        if (i in t_requete):\n",
        "            q=1\n",
        "        else:\n",
        "            q=0\n",
        "        Wq=Wq+(q*q)\n",
        "    Wq=math.sqrt(Wq)\n",
        "    for i in t:\n",
        "        try:\n",
        "            wd=Poids(i,file)\n",
        "        except:\n",
        "            wd=0\n",
        "        wdd=wdd+(wd*wd)\n",
        "    wdd=math.sqrt(wdd)\n",
        "    try:\n",
        "        RSV=Wd/(Wq*wdd)\n",
        "    except:\n",
        "        RSV=0\n",
        "    return RSV"
      ],
      "metadata": {
        "id": "Cs3U1stclvf5"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "#lecture de la requete\n",
        "requete=lecture_req()\n",
        "\n",
        "# Open a file\n",
        "path = \"sample_data/corpus/\"\n",
        "dirs = os.listdir( path )\n",
        "t=[]\n",
        "res={}\n",
        "ls=list()\n",
        "for file in dirs:\n",
        "  t=stem_function(file)\n",
        "  ls=ls+t\n",
        "ls=set(ls)  \n",
        "for file in dirs:\n",
        "  Ri=similarite(ls,file,requete)\n",
        "  res[file]=Ri\n",
        "r={k: v for k, v in sorted(res.items(), key=lambda item: item[1],reverse=True)}\n",
        "for i in r:\n",
        "    examen=i+\" : \"+str(r[i])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MbLf-Jdmkdu",
        "outputId": "e310b9f6-8585-4450-b47a-e5fdb98afbd1"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrer la requete : \n",
            "a\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n",
            "0\n",
            "0\n",
            "{}\n",
            "{'a': 0.0}\n",
            "{1: {'a': 0.0}}\n",
            "0\n",
            "0\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}